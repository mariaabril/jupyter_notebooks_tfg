{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maria\\anaconda3\\lib\\site-packages\\requests\\__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, DoubleType, DateType,TimestampType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SQLContext\n",
    "\n",
    "import json \n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import os\n",
    "from IPython.core.display import display, HTML\n",
    "from datetime import datetime\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"TFG\").getOrCreate()\n",
    "elastic = Elasticsearch(hosts=['http://127.0.0.1:9200/'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCHEMAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_vehicle = StructType([ \\\n",
    "    StructField(\"DEVICE_ID\",StringType(),True), \\\n",
    "    StructField(\"LATITUDE\",DoubleType(),True), \\\n",
    "    StructField(\"LONGITUDE\",DoubleType(),True), \\\n",
    "    StructField(\"DATETIME\",TimestampType(),True), \\\n",
    "    StructField(\"SPEED\",DoubleType(),True), \\\n",
    "  ])\n",
    "schema_excep =  StructType([ \\\n",
    "    StructField(\"DEVICE_ID\",StringType(),True), \\\n",
    "    StructField(\"RULE_ID\",StringType(),True), \\\n",
    "    StructField(\"ACTIVE_FROM\",TimestampType(),True), \\\n",
    "    StructField(\"ACTIVE_TO\",TimestampType(),True), \\\n",
    "    StructField(\"DURATION\",StringType(),True),\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEVICE_ID: string (nullable = true)\n",
      " |-- RULE_ID: string (nullable = true)\n",
      " |-- ACTIVE_FROM: timestamp (nullable = true)\n",
      " |-- ACTIVE_TO: timestamp (nullable = true)\n",
      " |-- DURATION: string (nullable = true)\n",
      "\n",
      "+---------+--------------+-----------------------+-----------------------+----------------+\n",
      "|DEVICE_ID|RULE_ID       |ACTIVE_FROM            |ACTIVE_TO              |DURATION        |\n",
      "+---------+--------------+-----------------------+-----------------------+----------------+\n",
      "|NU5CJSDX |RuleSeatbeltId|2021-02-01 01:17:30    |2021-02-01 01:17:38.66 |00:00:08.6600000|\n",
      "|4BF9LOXT |RuleSeatbeltId|2021-02-01 02:13:29.317|2021-02-01 02:14:07.103|00:00:37.7860000|\n",
      "|GT42SJ4M |RuleSeatbeltId|2021-02-01 02:22:07.183|2021-02-01 02:22:11    |00:00:03.8170000|\n",
      "+---------+--------------+-----------------------+-----------------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_look_csv = 'data/exception_original/exception_events_20210201_20210201_0_anon.csv' \n",
    "\n",
    "first_look_csv = spark.read.csv(first_look_csv, header=True, inferSchema=True, sep='|', schema=schema_excep)\n",
    "\n",
    "first_look_csv.printSchema()\n",
    "first_look_csv.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMMON FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_month_year(filename):\n",
    "    '''\n",
    "    return: day, month and year of the filename\n",
    "    '''\n",
    "    dia = filename.split('_')[1][6:]\n",
    "    mes = filename.split('_')[1][4:6]\n",
    "    ano = filename.split('_')[1][:4]\n",
    "    \n",
    "    return dia, mes, ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(lat1deg, lon1deg, lat2deg, lon2deg):\n",
    "    '''\n",
    "    return: distance in metres from 2 locations (lat, lon)\n",
    "    '''\n",
    "    if not all((lat1deg, lon1deg, lat2deg, lon2deg)):\n",
    "        return 0.0\n",
    "    \n",
    "    #approximate radius of earth in m\n",
    "    R = 6373000.0\n",
    "\n",
    "    lat1 = math.radians(lat1deg)\n",
    "    lon1 = math.radians(lon1deg)\n",
    "    lat2 = math.radians(lat2deg)\n",
    "    lon2 = math.radians(lon2deg)\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "# register as a UDF \n",
    "get_distance_udf = F.udf(get_distance, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_time(timestr): \n",
    "    '''\n",
    "    return: deltatime as float in seconds from HH:MM:SS.mmm\n",
    "    '''\n",
    "    values = timestr.split(':')\n",
    "    #Generate a timedelta\n",
    "    delta = timedelta(hours=float(values[0]), minutes=float(values[1]), seconds=float(values[2]))\n",
    "    #Represent in Seconds\n",
    "    return delta.total_seconds()\n",
    "\n",
    "# register as a UDF \n",
    "get_delta_time_udf = F.udf(get_delta_time, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_time_delta(y,x): \n",
    "    '''\n",
    "    return: duration in seconds\n",
    "    '''\n",
    "    if not all((x, y)):\n",
    "        return 0.0\n",
    "    delta = math.fabs((x-y).total_seconds())\n",
    "    return delta\n",
    "\n",
    "# register as a UDF \n",
    "abs_time_delta_udf = F.udf(abs_time_delta, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return time - 10 minutes\n",
    "mintimeudf = F.udf(lambda x: (x - timedelta(seconds=600)), TimestampType())\n",
    "# return time + 10 minutes\n",
    "maxtimeudf = F.udf(lambda x: (x + timedelta(seconds=600)), TimestampType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXCEPTIONS TREATMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exceptions(file, df_vehicle):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    df_excep_initial = spark.read.csv(file ,header=True, inferSchema=True, sep='|', schema=schema_excep) \\\n",
    "                    .withColumnRenamed('DURATION', 'DURATION_STR')\n",
    "\n",
    "    df_excep1 = df_excep_initial.withColumn('DATE', F.date_format(F.col('ACTIVE_FROM'), \"yyyy/MM/dd\")) \\\n",
    "                                .withColumn('DURATION', get_delta_time_udf(F.col('DURATION_STR'))) \\\n",
    "                                .withColumn('starttime', mintimeudf(F.col('ACTIVE_FROM'))) \\\n",
    "                                .withColumn('endtime', maxtimeudf(F.col('ACTIVE_TO'))) \\\n",
    "                                .drop('DURATION_STR')\n",
    "    df_excep2 = df_excep1.join(df_vehicle, on=((df_vehicle.DEVICE_ID==df_excep1.DEVICE_ID) &\n",
    "                                         (df_vehicle.DATETIME_V.between(df_excep1.starttime, df_excep1.endtime))), how='left') \\\n",
    "                         .drop(df_vehicle.DEVICE_ID)\n",
    "   \n",
    "    df_excep3 = df_excep2.where(F.col('DATETIME_V').isNotNull())\n",
    "    window_excep = Window.partitionBy([\"DEVICE_ID\", \"ACTIVE_FROM\"]).orderBy(F.col('diff_date').desc())\n",
    "    df_excep4 = df_excep3.withColumn('diff_date', abs_time_delta_udf(F.col('ACTIVE_FROM'), F.col('DATETIME_V'))) \\\n",
    "                        .withColumn('next_diff_date', F.lead('diff_date', default=1000).over(window_excep)) \\\n",
    "                        .withColumn('result', F.when(F.col('diff_date') < F.col('next_diff_date'), 1).otherwise(0)) \\\n",
    "                        .filter(F.col('result') == 1) \\\n",
    "                        .drop('diff_date', 'next_diff_date', 'result', 'DATETIME_V', 'ACTIVE_FROM', 'ACTIVE_TO', 'DURATION', 'SPEED')\n",
    "\n",
    "    return df_excep4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logrecords_20210201_20210201_0_anon.csv read at 2022-03-23 19:15:53.290354\n",
      "logrecords_20210202_20210202_0_anon.csv read at 2022-03-23 19:16:31.494057\n",
      "logrecords_20210203_20210203_0_anon.csv read at 2022-03-23 19:17:07.958469\n",
      "logrecords_20210204_20210204_0_anon.csv read at 2022-03-23 19:17:49.606970\n",
      "logrecords_20210205_20210205_0_anon.csv read at 2022-03-23 19:18:25.060419\n",
      "logrecords_20210206_20210206_0_anon.csv read at 2022-03-23 19:19:02.334725\n",
      "logrecords_20210207_20210207_0_anon.csv read at 2022-03-23 19:19:05.977840\n",
      "logrecords_20210208_20210208_0_anon.csv read at 2022-03-23 19:19:09.062481\n",
      "logrecords_20210209_20210209_0_anon.csv read at 2022-03-23 19:19:43.874609\n",
      "logrecords_20210210_20210210_0_anon.csv read at 2022-03-23 19:20:19.429809\n",
      "logrecords_20210211_20210211_0_anon.csv read at 2022-03-23 19:20:55.773540\n",
      "logrecords_20210212_20210212_0_anon.csv read at 2022-03-23 19:21:33.001012\n",
      "logrecords_20210213_20210213_0_anon.csv read at 2022-03-23 19:22:08.332846\n",
      "logrecords_20210214_20210214_0_anon.csv read at 2022-03-23 19:22:12.068541\n",
      "logrecords_20210215_20210215_0_anon.csv read at 2022-03-23 19:22:15.468384\n",
      "logrecords_20210216_20210216_0_anon.csv read at 2022-03-23 19:22:51.935130\n",
      "logrecords_20210217_20210217_0_anon.csv read at 2022-03-23 19:23:27.974264\n",
      "logrecords_20210218_20210218_0_anon.csv read at 2022-03-23 19:24:07.086426\n",
      "logrecords_20210219_20210219_0_anon.csv read at 2022-03-23 19:24:43.622191\n",
      "logrecords_20210220_20210220_0_anon.csv read at 2022-03-23 19:25:21.366256\n",
      "logrecords_20210221_20210221_0_anon.csv read at 2022-03-23 19:25:24.407882\n",
      "logrecords_20210222_20210222_0_anon.csv read at 2022-03-23 19:25:27.404077\n",
      "logrecords_20210223_20210223_0_anon.csv read at 2022-03-23 19:26:02.017959\n",
      "logrecords_20210224_20210224_0_anon.csv read at 2022-03-23 19:26:42.794899\n",
      "logrecords_20210225_20210225_0_anon.csv read at 2022-03-23 19:27:23.326466\n",
      "logrecords_20210226_20210226_0_anon.csv read at 2022-03-23 19:28:01.927034\n",
      "logrecords_20210227_20210227_0_anon.csv read at 2022-03-23 19:28:37.817523\n",
      "logrecords_20210228_20210228_0_anon.csv read at 2022-03-23 19:28:40.822350\n"
     ]
    }
   ],
   "source": [
    "MAIN_START_TIME = datetime.now()\n",
    "directory = \"data/fleet_original/\"\n",
    "for filename in os.listdir(directory):\n",
    "    # read csv\n",
    "    if filename.endswith(\".csv\"):\n",
    "        \n",
    "        dia, mes, ano =  get_day_month_year(filename)\n",
    "      \n",
    "        #---------------------\n",
    "        df_vehicles = spark.read.csv(directory+filename, header=True,inferSchema=True, sep='|',schema=schema_vehicle\n",
    "        df_exceptions = get_exceptions(directory_exceptions + exception_filename, df_vehicles)\n",
    "        \n",
    "        data_exceptions = df_exceptions.collect()\n",
    "\n",
    "        for row in data_exceptions:\n",
    "            elastic.index(index='exception_index',\n",
    "                             document={'device_id': row['DEVICE_ID'],\n",
    "                                        'date': row['DATE'],\n",
    "                                        'coordinates': [row['LONGITUDE'], row['LATITUDE']]})\n",
    "            \n",
    "print('exception_treatment process finished (duration = {} hours, {} minutes)'.format(\n",
    "    ((datetime.now() - MAIN_START_TIME).seconds)//3600,\n",
    "    (((datetime.now() - MAIN_START_TIME).seconds)//60)%60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
