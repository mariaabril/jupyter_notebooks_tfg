{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType,StructField, StringType, DoubleType, DateType,TimestampType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SQLContext\n",
    "\n",
    "import json \n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import os\n",
    "from IPython.core.display import display, HTML\n",
    "from datetime import datetime\n",
    "import pyspark.conf as conf\n",
    "conf.autoBroadcastJoinThreshold = -1 # this parameters avoids stopping the program when it takes too long\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"TFG\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCHEMAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_vehicle = StructType([ \\\n",
    "    StructField(\"DEVICE_ID\",StringType(),True), \\\n",
    "    StructField(\"LATITUDE\",DoubleType(),True), \\\n",
    "    StructField(\"LONGITUDE\",DoubleType(),True), \\\n",
    "    StructField(\"DATETIME\",TimestampType(),True), \\\n",
    "    StructField(\"SPEED\",DoubleType(),True), \\\n",
    "  ])\n",
    "schema_fuel = StructType([ \\\n",
    "    StructField(\"DEVICE_ID\",StringType(),True), \\\n",
    "    StructField(\"LITRES\",DoubleType(),True), \\\n",
    "    StructField(\"DATETIME\",TimestampType(),True)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEVICE_ID: string (nullable = true)\n",
      " |-- LITRES: double (nullable = true)\n",
      " |-- DATETIME: timestamp (nullable = true)\n",
      "\n",
      "+---------+------------------+-----------------------+\n",
      "|DEVICE_ID|LITRES            |DATETIME               |\n",
      "+---------+------------------+-----------------------+\n",
      "|4BF9LOXT |2413.4900000000002|2021-02-01 01:00:18.147|\n",
      "|QI9IXIGJ |696.38            |2021-02-01 01:00:05.02 |\n",
      "|QI9IXIGJ |696.38            |2021-02-01 01:00:06.023|\n",
      "+---------+------------------+-----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_look_csv = 'data/fuel_original/fuel_20210201_20210201_0_anon.csv' \n",
    "\n",
    "first_look_csv = spark.read.csv(first_look_csv, header=True, inferSchema=True, sep='|', schema=schema_fuel)\n",
    "\n",
    "first_look_csv.printSchema()\n",
    "first_look_csv.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMMON FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(lat1deg, lon1deg, lat2deg, lon2deg):\n",
    "    '''\n",
    "    return: distance in metres from 2 locations (lat, lon)\n",
    "    '''\n",
    "    if not all((lat1deg, lon1deg, lat2deg, lon2deg)):\n",
    "        return 0.0\n",
    "    \n",
    "    #approximate radius of earth in m\n",
    "    R = 6373000.0\n",
    "\n",
    "    lat1 = math.radians(lat1deg)\n",
    "    lon1 = math.radians(lon1deg)\n",
    "    lat2 = math.radians(lat2deg)\n",
    "    lon2 = math.radians(lon2deg)\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "# register as a UDF \n",
    "get_distance_udf = F.udf(get_distance, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_month_year(filename):\n",
    "    '''\n",
    "    return: day, month and year of the filename\n",
    "    '''\n",
    "    dia = filename.split('_')[1][6:]\n",
    "    mes = filename.split('_')[1][4:6]\n",
    "    ano = filename.split('_')[1][:4]\n",
    "    \n",
    "    return dia, mes, ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_time(timestr): \n",
    "    '''\n",
    "    return: deltatime as float in seconds from HH:MM:SS.mmm\n",
    "    '''\n",
    "    values = timestr.split(':')\n",
    "    #Generate a timedelta\n",
    "    delta = timedelta(hours=float(values[0]), minutes=float(values[1]), seconds=float(values[2]))\n",
    "    #Represent in Seconds\n",
    "    return delta.total_seconds()\n",
    "\n",
    "# register as a UDF \n",
    "get_delta_time_udf = F.udf(get_delta_time, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_time_delta(y,x): \n",
    "    '''\n",
    "    return: duration in seconds\n",
    "    '''\n",
    "    if not all((x, y)):\n",
    "        return 0.0\n",
    "    delta = math.fabs((x-y).total_seconds())\n",
    "    return delta\n",
    "\n",
    "# register as a UDF \n",
    "abs_time_delta_udf = F.udf(abs_time_delta, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return time - 31 seconds\n",
    "mintimeudf = F.udf(lambda x: (x - timedelta(seconds=31)), TimestampType())\n",
    "# return time + 31 seconds\n",
    "maxtimeudf = F.udf(lambda x: (x + timedelta(seconds=31)), TimestampType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUEL TREATMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fuel(file, df_vehicle):\n",
    "    df_fuel_initial = spark.read.csv(file, header=True,inferSchema=True, sep='|',schema=schema_fuel\n",
    "                                    ).withColumnRenamed('DATETIME', 'DATETIME_F')\n",
    "    df_fuel1 = df_fuel_initial.withColumn('prev_fuel', F.lag('LITRES').over(Window.partitionBy(\"DEVICE_ID\").orderBy('DATETIME_F'))) \\\n",
    "                        .withColumn('diff_fuel', F.when(F.isnull(F.col('prev_fuel')),0).otherwise(F.col('LITRES')-F.col('prev_fuel'))) \\\n",
    "                        .withColumn('start_time', mintimeudf(F.col('DATETIME_F'))) \\\n",
    "                        .withColumn('end_time', maxtimeudf(F.col('DATETIME_F'))) \\\n",
    "                        .drop('prev_fuel')\n",
    "    \n",
    "    \n",
    "    df_fuel2 = df_fuel1.join(df_vehicle, on=((df_vehicle.DEVICE_ID==df_fuel1.DEVICE_ID) &\n",
    "                                         (df_vehicle.DATETIME_V.between(df_fuel1.start_time, df_fuel1.end_time))), how='left') \\\n",
    "                    .drop('start_time', 'end_time') \\\n",
    "                    .drop(df_vehicle.DEVICE_ID) \\\n",
    "                    .cache()\n",
    "    #df_fuel_no_date = df_fuel2.where(F.col('DATETIME_V').isNull())\n",
    "    df_fuel3 = df_fuel2.where(F.col('DATETIME_V').isNotNull())\n",
    "    window_fuel = Window.partitionBy([\"DEVICE_ID\", \"DATETIME_F\"]).orderBy(F.col('diff_date').desc())\n",
    "    df_fuel4 = df_fuel3.withColumn('diff_date', abs_time_delta_udf(F.col('DATETIME_F'), F.col('DATETIME_V'))) \\\n",
    "                    .withColumn('next_diff_date', F.lead('diff_date', default=1000).over(window_fuel)) \\\n",
    "                    .withColumn('result', F.when(F.col('diff_date') < F.col('next_diff_date'), 1).otherwise(0)) \\\n",
    "                    .filter(F.col('result') == 1) \\\n",
    "                    .drop('diff_date', 'next_diff_date', 'result')\n",
    "    df_fuel5 = df_fuel4.withColumn('PREV_LATITUDE', F.lag('LATITUDE').over(Window.partitionBy(\"DEVICE_ID\").orderBy('DATETIME_F'))) \\\n",
    "                    .withColumn('PREV_LONGITUDE', F.lag('LONGITUDE').over(Window.partitionBy(\"DEVICE_ID\").orderBy('DATETIME_F'))) \\\n",
    "                    .withColumn('DISTANCE', get_distance_udf(F.col('LATITUDE'), F.col('LONGITUDE'), F.col('PREV_LATITUDE'), F.col('PREV_LONGITUDE'))) \\\n",
    "                    .select('DEVICE_ID','LITRES','DATETIME_F','diff_fuel','SPEED','DISTANCE') \\\n",
    "                    .cache()\n",
    "    \n",
    "    return df_fuel5\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE FUEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fuel(df_fuel, output_file):\n",
    "    data_fuel = df_fuel.rdd.toLocalIterator()\n",
    "    primera_linea = True\n",
    "    for row in data_fuel:\n",
    "        if primera_linea:\n",
    "            primera_linea = False\n",
    "        else:\n",
    "            output_file.write(', ')\n",
    "        date = '{}/{}/{}'.format(row['DATETIME_F'].strftime(\"%Y\"),row['DATETIME_F'].strftime(\"%m\"),row['DATETIME_F'].strftime(\"%d\"))\n",
    "        output_file.write('{\"device_id\": \"' + row['DEVICE_ID'] \n",
    "                          + '\", \"date\": \"' + date\n",
    "                          + '\", \"diff_fuel\": \"' + str(row['diff_fuel'])\n",
    "                          #+ '\", \"speed\": \"' + str(row['SPEED']) \n",
    "                          + '\", \"distance\": \"' + str(row['DISTANCE']) + '\"}')\n",
    "    \n",
    "    spark.catalog.clearCache  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logrecords_20210201_20210201_0_anon.csv read at 2022-02-16 15:05:41.464618\n",
      "df_fuel loaded at 2022-02-16 15:05:43.399125\n"
     ]
    }
   ],
   "source": [
    "MAIN_START_TIME = datetime.now()\n",
    "directory = \"data/fleet_original/\"\n",
    "starting_time = datetime.now()\n",
    "for filename in os.listdir(directory):\n",
    "    # leemos csv\n",
    "    if filename.endswith(\".csv\"):\n",
    "        \n",
    "        dia, mes, ano =  get_day_month_year(filename)\n",
    "        \n",
    "        #---------------------\n",
    "        df_vehicles = spark.read.csv(directory+filename, header=True,inferSchema=True, sep='|',schema=schema_vehicle\n",
    "                                    ).withColumnRenamed('DATETIME', 'DATETIME_V')\n",
    "        print('{} read at {}'.format(filename, datetime.now()))\n",
    "\n",
    "        #---------------------\n",
    "        output_file = open('json_data/fuel/fuel_{}{}{}.json'.format(ano,mes,dia,ano,mes,dia), \"wt\")\n",
    "        output_file.write('{\"type\": \"FeatureCollection\", \"features\": [')\n",
    "        \n",
    "        directory_fuel = \"data/fuel_original/\"\n",
    "        fuel_filename = 'fuel_{}{}{}_{}{}{}_0_anon.csv'.format(ano,mes,dia,ano,mes,dia)\n",
    "        df_fuel = get_fuel(directory_fuel + fuel_filename, df_vehicles)\n",
    "        print('df_fuel loaded at {}'.format(datetime.now()))\n",
    "        save_fuel(df_fuel, output_file)\n",
    "        print('save fuel done at {}'.format(datetime.now()))\n",
    "        output_file.write(']}')\n",
    "        output_file.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('fuel_treatment process finished (duration = {} hours, {} minutes)'.format(\n",
    "    ((datetime.now() - MAIN_START_TIME).seconds)//3600,\n",
    "    (((datetime.now() - MAIN_START_TIME).seconds)//60)%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruebas memoria "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|DEVICE_ID|            LITRES|          DATETIME_F|\n",
      "+---------+------------------+--------------------+\n",
      "| 4BF9LOXT|2413.4900000000002|2021-02-01 01:00:...|\n",
      "| QI9IXIGJ|            696.38|2021-02-01 01:00:...|\n",
      "| QI9IXIGJ|            696.38|2021-02-01 01:00:...|\n",
      "| QI9IXIGJ|            696.39|2021-02-01 01:00:...|\n",
      "| QI9IXIGJ|            696.39|2021-02-01 01:00:...|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+------+--------------------+---------+--------------------+--------------------+\n",
      "|DEVICE_ID|LITRES|          DATETIME_F|diff_fuel|          start_time|            end_time|\n",
      "+---------+------+--------------------+---------+--------------------+--------------------+\n",
      "| 4XUR6E8S|852.54|2021-02-01 07:36:...|      0.0|2021-02-01 07:35:...|2021-02-01 07:36:...|\n",
      "| 4XUR6E8S|852.54|2021-02-01 07:36:...|      0.0|2021-02-01 07:35:...|2021-02-01 07:36:...|\n",
      "| 4XUR6E8S|852.54|2021-02-01 07:36:...|      0.0|2021-02-01 07:35:...|2021-02-01 07:37:...|\n",
      "| 4XUR6E8S|852.54|2021-02-01 07:36:...|      0.0|2021-02-01 07:36:...|2021-02-01 07:37:...|\n",
      "| 4XUR6E8S|852.54|2021-02-01 07:36:...|      0.0|2021-02-01 07:36:...|2021-02-01 07:37:...|\n",
      "+---------+------+--------------------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+------+--------------------+---------+---------+-----------+--------------------+-----+\n",
      "|DEVICE_ID|LITRES|          DATETIME_F|diff_fuel| LATITUDE|  LONGITUDE|          DATETIME_V|SPEED|\n",
      "+---------+------+--------------------+---------+---------+-----------+--------------------+-----+\n",
      "| 4XUR6E8S|852.54|2021-02-01 07:36:...|      0.0|36.663784|-4.55500793|2021-02-01 07:36:...|  0.0|\n",
      "| 4XUR6E8S|852.54|2021-02-01 07:36:...|      0.0|36.663784|-4.55500793| 2021-02-01 07:36:04|  0.0|\n",
      "| 4XUR6E8S|852.54|2021-02-01 07:36:...|      0.0|36.663784|-4.55500793| 2021-02-01 07:36:23|  1.0|\n",
      "| 4XUR6E8S|852.54|2021-02-01 07:36:...|      0.0|36.663784|-4.55500793| 2021-02-01 07:36:30|  0.0|\n",
      "| 4XUR6E8S|852.54|2021-02-01 07:36:...|      0.0|36.663784|-4.55500793| 2021-02-01 07:36:40|  1.0|\n",
      "+---------+------+--------------------+---------+---------+-----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+------------------+--------------------+--------------------+----------+------------+--------------------+-----+\n",
      "|DEVICE_ID|            LITRES|          DATETIME_F|           diff_fuel|  LATITUDE|   LONGITUDE|          DATETIME_V|SPEED|\n",
      "+---------+------------------+--------------------+--------------------+----------+------------+--------------------+-----+\n",
      "| 00H12ZKQ|            205.41|2021-02-01 16:29:...|0.009999999999990905| 38.996479|-0.522598505|2021-02-01 16:29:...|  0.0|\n",
      "| 00H12ZKQ|205.64000000000001|2021-02-01 16:48:...|                 0.0|38.9964027|-0.522470474| 2021-02-01 16:48:25|  2.0|\n",
      "| 00MJTFIG|           3070.84|2021-02-01 12:30:...| 0.01999999999998181|43.1176186| -7.63000298| 2021-02-01 12:30:01|  1.0|\n",
      "| 05DW3NQ7|           5621.49|2021-02-01 15:05:...|                 0.0|36.5895424| -4.55313921| 2021-02-01 15:05:53|  0.0|\n",
      "| 08OWVC1O|           1597.75|2021-02-01 15:38:...| 0.12999999999988177|36.7256317| -4.46074867| 2021-02-01 15:38:02|  0.0|\n",
      "+---------+------------------+--------------------+--------------------+----------+------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+------+--------------------+---------+-----+--------+\n",
      "|DEVICE_ID|LITRES|          DATETIME_F|diff_fuel|SPEED|DISTANCE|\n",
      "+---------+------+--------------------+---------+-----+--------+\n",
      "| 4XUR6E8S|852.54|2021-02-01 07:36:...|      0.0|  1.0|     0.0|\n",
      "| 4XUR6E8S|852.54|2021-02-01 07:36:...|      0.0|  1.0|     0.0|\n",
      "| 4XUR6E8S|852.54|2021-02-01 07:36:...|      0.0|  0.0|     0.0|\n",
      "| 4XUR6E8S|852.54|2021-02-01 07:36:...|      0.0|  1.0|     0.0|\n",
      "| 4XUR6E8S|852.54|2021-02-01 07:36:...|      0.0|  0.0|     0.0|\n",
      "+---------+------+--------------------+---------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vehicle = spark.read.csv( \"datos/logrecord_febrero/logrecords_20210201_20210201_0_anon.csv\", header=True,inferSchema=True, sep='|',schema=schema_vehicle\n",
    "                                    ).withColumnRenamed('DATETIME', 'DATETIME_V')\n",
    "    \n",
    "df_fuel_initial = spark.read.csv('datos/fuel/fuel_20210201_20210201_0_anon.csv', header=True,inferSchema=True, sep='|',schema=schema_fuel\n",
    "                                ).withColumnRenamed('DATETIME', 'DATETIME_F')\n",
    "df_fuel_initial.show(5)\n",
    "\n",
    "df_fuel1 = df_fuel_initial.withColumn('prev_fuel', F.lag('LITRES').over(Window.partitionBy(\"DEVICE_ID\").orderBy('DATETIME_F'))) \\\n",
    "                    .withColumn('diff_fuel', F.when(F.isnull(F.col('prev_fuel')),0).otherwise(F.col('LITRES')-F.col('prev_fuel'))) \\\n",
    "                    .withColumn('start_time', mintimeudf(F.col('DATETIME_F'))) \\\n",
    "                    .withColumn('end_time', maxtimeudf(F.col('DATETIME_F'))) \\\n",
    "                    .drop('prev_fuel')\n",
    "\n",
    "df_fuel1.show(5)\n",
    "df_fuel2 = df_fuel1.join(df_vehicle, on=((df_vehicle.DEVICE_ID==df_fuel1.DEVICE_ID) &\n",
    "                                     (df_vehicle.DATETIME_V.between(df_fuel1.start_time, df_fuel1.end_time))), how='left') \\\n",
    "                .drop('start_time', 'end_time') \\\n",
    "                .drop(df_vehicle.DEVICE_ID) \\\n",
    "                .cache()\n",
    "\n",
    "df_fuel2.show(5)\n",
    "#df_fuel_no_date = df_fuel2.where(F.col('DATETIME_V').isNull())\n",
    "df_fuel3 = df_fuel2.where(F.col('DATETIME_V').isNotNull())\n",
    "window_fuel = Window.partitionBy([\"DEVICE_ID\", \"DATETIME_F\"]).orderBy(F.col('diff_date').desc())\n",
    "df_fuel4 = df_fuel3.withColumn('diff_date', abs_time_delta_udf(F.col('DATETIME_F'), F.col('DATETIME_V'))) \\\n",
    "                .withColumn('next_diff_date', F.lead('diff_date', default=1000).over(window_fuel)) \\\n",
    "                .withColumn('result', F.when(F.col('diff_date') < F.col('next_diff_date'), 1).otherwise(0)) \\\n",
    "                .filter(F.col('result') == 1) \\\n",
    "                .drop('diff_date', 'next_diff_date', 'result')\n",
    "\n",
    "df_fuel4.show(5)\n",
    "df_fuel5 = df_fuel4.withColumn('PREV_LATITUDE', F.lag('LATITUDE').over(Window.partitionBy(\"DEVICE_ID\").orderBy('DATETIME_F'))) \\\n",
    "                .withColumn('PREV_LONGITUDE', F.lag('LONGITUDE').over(Window.partitionBy(\"DEVICE_ID\").orderBy('DATETIME_F'))) \\\n",
    "                .withColumn('DISTANCE', get_distance_udf(F.col('LATITUDE'), F.col('LONGITUDE'), F.col('PREV_LATITUDE'), F.col('PREV_LONGITUDE'))) \\\n",
    "                .select('DEVICE_ID','LITRES','DATETIME_F','diff_fuel','SPEED','DISTANCE') \\\n",
    "                .cache()\n",
    "df_fuel5.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
