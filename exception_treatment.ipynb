{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, DoubleType, DateType,TimestampType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SQLContext\n",
    "\n",
    "import json \n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import os\n",
    "from IPython.core.display import display, HTML\n",
    "from datetime import datetime\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"TFG\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCHEMAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_vehicle = StructType([ \\\n",
    "    StructField(\"DEVICE_ID\",StringType(),True), \\\n",
    "    StructField(\"LATITUDE\",DoubleType(),True), \\\n",
    "    StructField(\"LONGITUDE\",DoubleType(),True), \\\n",
    "    StructField(\"DATETIME\",TimestampType(),True), \\\n",
    "    StructField(\"SPEED\",DoubleType(),True), \\\n",
    "  ])\n",
    "schema_excep =  StructType([ \\\n",
    "    StructField(\"DEVICE_ID\",StringType(),True), \\\n",
    "    StructField(\"RULE_ID\",StringType(),True), \\\n",
    "    StructField(\"ACTIVE_FROM\",TimestampType(),True), \\\n",
    "    StructField(\"ACTIVE_TO\",TimestampType(),True), \\\n",
    "    StructField(\"DURATION\",StringType(),True),\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEVICE_ID: string (nullable = true)\n",
      " |-- RULE_ID: string (nullable = true)\n",
      " |-- ACTIVE_FROM: timestamp (nullable = true)\n",
      " |-- ACTIVE_TO: timestamp (nullable = true)\n",
      " |-- DURATION: string (nullable = true)\n",
      "\n",
      "+---------+--------------+-----------------------+-----------------------+----------------+\n",
      "|DEVICE_ID|RULE_ID       |ACTIVE_FROM            |ACTIVE_TO              |DURATION        |\n",
      "+---------+--------------+-----------------------+-----------------------+----------------+\n",
      "|NU5CJSDX |RuleSeatbeltId|2021-02-01 01:17:30    |2021-02-01 01:17:38.66 |00:00:08.6600000|\n",
      "|4BF9LOXT |RuleSeatbeltId|2021-02-01 02:13:29.317|2021-02-01 02:14:07.103|00:00:37.7860000|\n",
      "|GT42SJ4M |RuleSeatbeltId|2021-02-01 02:22:07.183|2021-02-01 02:22:11    |00:00:03.8170000|\n",
      "+---------+--------------+-----------------------+-----------------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_look_csv = 'data/exception_original/exception_events_20210201_20210201_0_anon.csv' \n",
    "\n",
    "first_look_csv = spark.read.csv(first_look_csv, header=True, inferSchema=True, sep='|', schema=schema_excep)\n",
    "\n",
    "first_look_csv.printSchema()\n",
    "first_look_csv.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMON FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_month_year(filename):\n",
    "    '''\n",
    "    return: day, month and year of the filename\n",
    "    '''\n",
    "    dia = filename.split('_')[1][6:]\n",
    "    mes = filename.split('_')[1][4:6]\n",
    "    ano = filename.split('_')[1][:4]\n",
    "    \n",
    "    return dia, mes, ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(lat1deg, lon1deg, lat2deg, lon2deg):\n",
    "    '''\n",
    "    return: distance in metres from 2 locations (lat, lon)\n",
    "    '''\n",
    "    if not all((lat1deg, lon1deg, lat2deg, lon2deg)):\n",
    "        return 0.0\n",
    "    \n",
    "    #approximate radius of earth in m\n",
    "    R = 6373000.0\n",
    "\n",
    "    lat1 = math.radians(lat1deg)\n",
    "    lon1 = math.radians(lon1deg)\n",
    "    lat2 = math.radians(lat2deg)\n",
    "    lon2 = math.radians(lon2deg)\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "# register as a UDF \n",
    "get_distance_udf = F.udf(get_distance, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_time(timestr): \n",
    "    '''\n",
    "    return: deltatime as float in seconds from HH:MM:SS.mmm\n",
    "    '''\n",
    "    values = timestr.split(':')\n",
    "    #Generate a timedelta\n",
    "    delta = timedelta(hours=float(values[0]), minutes=float(values[1]), seconds=float(values[2]))\n",
    "    #Represent in Seconds\n",
    "    return delta.total_seconds()\n",
    "\n",
    "# register as a UDF \n",
    "get_delta_time_udf = F.udf(get_delta_time, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_time_delta(y,x): \n",
    "    '''\n",
    "    return: duration in seconds\n",
    "    '''\n",
    "    if not all((x, y)):\n",
    "        return 0.0\n",
    "    delta = math.fabs((x-y).total_seconds())\n",
    "    return delta\n",
    "\n",
    "# register as a UDF \n",
    "abs_time_delta_udf = F.udf(abs_time_delta, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return time - 10 minutes\n",
    "mintimeudf = F.udf(lambda x: (x - timedelta(seconds=600)), TimestampType())\n",
    "# return time + 10 minutes\n",
    "maxtimeudf = F.udf(lambda x: (x + timedelta(seconds=600)), TimestampType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXCEPTIONS TREATMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exceptions(file, df_vehicle):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    df_excep_initial = spark.read.csv(file ,header=True, inferSchema=True, sep='|', schema=schema_excep) \\\n",
    "                    .withColumnRenamed('DURATION', 'DURATION_STR')\n",
    "\n",
    "    df_excep1 = df_excep_initial.withColumn('DATE', F.date_format(F.col('ACTIVE_FROM'), \"yyyy/MM/dd\")) \\\n",
    "                                .withColumn('DURATION', get_delta_time_udf(F.col('DURATION_STR'))) \\\n",
    "                                .withColumn('starttime', mintimeudf(F.col('ACTIVE_FROM'))) \\\n",
    "                                .withColumn('endtime', maxtimeudf(F.col('ACTIVE_TO'))) \\\n",
    "                                .drop('DURATION_STR')\n",
    "    df_excep2 = df_excep1.join(df_vehicle, on=((df_vehicle.DEVICE_ID==df_excep1.DEVICE_ID) &\n",
    "                                         (df_vehicle.DATETIME_V.between(df_excep1.starttime, df_excep1.endtime))), how='left') \\\n",
    "                         .drop(df_vehicle.DEVICE_ID)\n",
    "   \n",
    "    df_excep3 = df_excep2.where(F.col('DATETIME_V').isNotNull())\n",
    "    window_excep = Window.partitionBy([\"DEVICE_ID\", \"ACTIVE_FROM\"]).orderBy(F.col('diff_date').desc())\n",
    "    df_excep4 = df_excep3.withColumn('diff_date', abs_time_delta_udf(F.col('ACTIVE_FROM'), F.col('DATETIME_V'))) \\\n",
    "                        .withColumn('next_diff_date', F.lead('diff_date', default=1000).over(window_excep)) \\\n",
    "                        .withColumn('result', F.when(F.col('diff_date') < F.col('next_diff_date'), 1).otherwise(0)) \\\n",
    "                        .filter(F.col('result') == 1) \\\n",
    "                        .drop('diff_date', 'next_diff_date', 'result', 'DATETIME_V', 'ACTIVE_FROM', 'ACTIVE_TO', 'DURATION', 'SPEED')\n",
    "\n",
    "    return df_excep4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE EXCEPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_exceptions(df_exceptions, output_file):\n",
    "    '''\n",
    "    defines the structure the json file will take\n",
    "    '''\n",
    "    data_exceptions = df_exceptions.collect()\n",
    "    primera_linea = True\n",
    "    for row in data_exceptions:\n",
    "        if primera_linea:\n",
    "            primera_linea = False\n",
    "        else:\n",
    "            output_file.write(', ')\n",
    "        output_file.write('{\"device_id\": \"' + row['DEVICE_ID'] \n",
    "                          + '\", \"date\": \"' + row['DATE'] \n",
    "                          + '\", \"type_exception\": \"' + row['RULE_ID'] \n",
    "                          + '\", \"coordinates\": [' + str(row['LONGITUDE']) + ', ' + str(row['LATITUDE']) + ']}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logrecords_20210201_20210201_0_anon.csv read at 2022-02-15 19:54:51.107084\n",
      "df_exceptions done at 2022-02-15 19:54:52.368220\n",
      "save exceptions done at 2022-02-15 19:55:39.763834\n",
      "logrecords_20210202_20210202_0_anon.csv read at 2022-02-15 19:55:39.960784\n",
      "df_exceptions done at 2022-02-15 19:55:41.982997\n",
      "save exceptions done at 2022-02-15 19:56:13.850699\n",
      "logrecords_20210203_20210203_0_anon.csv read at 2022-02-15 19:56:13.881597\n",
      "df_exceptions done at 2022-02-15 19:56:14.088630\n",
      "save exceptions done at 2022-02-15 19:56:43.749325\n",
      "logrecords_20210204_20210204_0_anon.csv read at 2022-02-15 19:56:43.773228\n",
      "df_exceptions done at 2022-02-15 19:56:43.960995\n",
      "save exceptions done at 2022-02-15 19:57:08.320266\n",
      "logrecords_20210205_20210205_0_anon.csv read at 2022-02-15 19:57:08.337050\n",
      "df_exceptions done at 2022-02-15 19:57:08.520410\n",
      "save exceptions done at 2022-02-15 19:57:34.135430\n",
      "logrecords_20210206_20210206_0_anon.csv read at 2022-02-15 19:57:34.162623\n",
      "df_exceptions done at 2022-02-15 19:57:34.365173\n",
      "save exceptions done at 2022-02-15 19:57:39.498892\n",
      "logrecords_20210207_20210207_0_anon.csv read at 2022-02-15 19:57:39.517429\n",
      "df_exceptions done at 2022-02-15 19:57:39.630130\n",
      "save exceptions done at 2022-02-15 19:57:44.015520\n",
      "logrecords_20210208_20210208_0_anon.csv read at 2022-02-15 19:57:44.035190\n",
      "df_exceptions done at 2022-02-15 19:57:44.168016\n",
      "save exceptions done at 2022-02-15 19:58:08.719429\n",
      "logrecords_20210209_20210209_0_anon.csv read at 2022-02-15 19:58:08.735422\n",
      "df_exceptions done at 2022-02-15 19:58:08.873077\n",
      "save exceptions done at 2022-02-15 19:58:34.293000\n",
      "logrecords_20210210_20210210_0_anon.csv read at 2022-02-15 19:58:34.304049\n",
      "df_exceptions done at 2022-02-15 19:58:34.473842\n",
      "save exceptions done at 2022-02-15 19:59:07.456756\n",
      "logrecords_20210211_20210211_0_anon.csv read at 2022-02-15 19:59:07.474474\n",
      "df_exceptions done at 2022-02-15 19:59:07.588275\n",
      "save exceptions done at 2022-02-15 19:59:34.640001\n",
      "logrecords_20210212_20210212_0_anon.csv read at 2022-02-15 19:59:34.656361\n",
      "df_exceptions done at 2022-02-15 19:59:34.804447\n",
      "save exceptions done at 2022-02-15 20:00:05.271308\n",
      "logrecords_20210213_20210213_0_anon.csv read at 2022-02-15 20:00:05.290426\n",
      "df_exceptions done at 2022-02-15 20:00:05.450799\n",
      "save exceptions done at 2022-02-15 20:00:11.682035\n",
      "logrecords_20210214_20210214_0_anon.csv read at 2022-02-15 20:00:11.704451\n",
      "df_exceptions done at 2022-02-15 20:00:11.937507\n",
      "save exceptions done at 2022-02-15 20:00:17.009505\n",
      "logrecords_20210215_20210215_0_anon.csv read at 2022-02-15 20:00:17.030030\n",
      "df_exceptions done at 2022-02-15 20:00:17.231305\n",
      "save exceptions done at 2022-02-15 20:00:52.725797\n",
      "logrecords_20210216_20210216_0_anon.csv read at 2022-02-15 20:00:52.744703\n",
      "df_exceptions done at 2022-02-15 20:00:52.907268\n",
      "save exceptions done at 2022-02-15 20:01:30.500068\n",
      "logrecords_20210217_20210217_0_anon.csv read at 2022-02-15 20:01:30.513904\n",
      "df_exceptions done at 2022-02-15 20:01:30.696923\n",
      "save exceptions done at 2022-02-15 20:02:09.266876\n",
      "logrecords_20210218_20210218_0_anon.csv read at 2022-02-15 20:02:09.286113\n",
      "df_exceptions done at 2022-02-15 20:02:09.442989\n",
      "save exceptions done at 2022-02-15 20:02:47.690316\n",
      "logrecords_20210219_20210219_0_anon.csv read at 2022-02-15 20:02:47.720647\n",
      "df_exceptions done at 2022-02-15 20:02:47.946891\n",
      "save exceptions done at 2022-02-15 20:03:27.524930\n",
      "logrecords_20210220_20210220_0_anon.csv read at 2022-02-15 20:03:27.545354\n",
      "df_exceptions done at 2022-02-15 20:03:27.777551\n",
      "save exceptions done at 2022-02-15 20:03:33.818420\n",
      "logrecords_20210221_20210221_0_anon.csv read at 2022-02-15 20:03:33.840148\n",
      "df_exceptions done at 2022-02-15 20:03:34.072452\n",
      "save exceptions done at 2022-02-15 20:03:39.535147\n",
      "logrecords_20210222_20210222_0_anon.csv read at 2022-02-15 20:03:39.559082\n",
      "df_exceptions done at 2022-02-15 20:03:39.747034\n",
      "save exceptions done at 2022-02-15 20:04:15.768416\n",
      "logrecords_20210223_20210223_0_anon.csv read at 2022-02-15 20:04:15.793613\n",
      "df_exceptions done at 2022-02-15 20:04:15.999461\n",
      "save exceptions done at 2022-02-15 20:04:53.723399\n",
      "logrecords_20210224_20210224_0_anon.csv read at 2022-02-15 20:04:53.751812\n",
      "df_exceptions done at 2022-02-15 20:04:53.964252\n",
      "save exceptions done at 2022-02-15 20:05:31.932364\n",
      "logrecords_20210225_20210225_0_anon.csv read at 2022-02-15 20:05:31.953887\n",
      "df_exceptions done at 2022-02-15 20:05:32.140025\n",
      "save exceptions done at 2022-02-15 20:06:09.439268\n",
      "logrecords_20210226_20210226_0_anon.csv read at 2022-02-15 20:06:09.448245\n",
      "df_exceptions done at 2022-02-15 20:06:09.654207\n",
      "save exceptions done at 2022-02-15 20:06:38.160722\n",
      "logrecords_20210227_20210227_0_anon.csv read at 2022-02-15 20:06:38.180037\n",
      "df_exceptions done at 2022-02-15 20:06:38.288942\n",
      "save exceptions done at 2022-02-15 20:06:43.532662\n",
      "logrecords_20210228_20210228_0_anon.csv read at 2022-02-15 20:06:43.532662\n",
      "df_exceptions done at 2022-02-15 20:06:43.697058\n",
      "save exceptions done at 2022-02-15 20:06:47.662845\n"
     ]
    }
   ],
   "source": [
    "MAIN_START_TIME = datetime.now()\n",
    "directory = \"data/fleet_original/\"\n",
    "for filename in os.listdir(directory):\n",
    "    # read csv\n",
    "    if filename.endswith(\".csv\"):\n",
    "        \n",
    "        dia, mes, ano =  get_day_month_year(filename)\n",
    "      \n",
    "        #---------------------\n",
    "        df_vehicles = spark.read.csv(directory+filename, header=True,inferSchema=True, sep='|',schema=schema_vehicle\n",
    "                                    ).withColumnRenamed('DATETIME', 'DATETIME_V') # read vehicles csv and rename datetimecolumn so we know it corresponds to the vehicles\n",
    "        print('{} read at {}'.format(filename, datetime.now()))\n",
    "\n",
    "        # open file\n",
    "        output_file = open(\"json_data/exceptions/exceptions_{}{}{}.json\".format(ano,mes,dia), \"wt\")\n",
    "       \n",
    "        output_file.write('{\"type\": \"FeatureCollection\", \"features\": [') # all the lines will start like this\n",
    "        \n",
    "        directory_exceptions = \"data/exception_original/\"\n",
    "        exception_filename = 'exception_events_{}{}{}_{}{}{}_0_anon.csv'.format(ano,mes,dia,ano,mes,dia)\n",
    "        \n",
    "        df_exceptions = get_exceptions(directory_exceptions + exception_filename, df_vehicles)\n",
    "        \n",
    "        print('df_exceptions done at {}'.format(datetime.now()))\n",
    "        save_exceptions(df_exceptions, output_file)\n",
    "    \n",
    "        output_file.write(']}')\n",
    "        output_file.close()\n",
    "        print('save exceptions done at {}'.format(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception_treatment process finished (duration = 0 hours, 11 minutes)\n"
     ]
    }
   ],
   "source": [
    "print('exception_treatment process finished (duration = {} hours, {} minutes)'.format(\n",
    "    ((datetime.now() - MAIN_START_TIME).seconds)//3600,\n",
    "    (((datetime.now() - MAIN_START_TIME).seconds)//60)%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
